---
title: "Week 9 Data Work"
output:
      html_document:
        keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(sjPlot)
library(finalfit)
library(knitr)
library(jtools)
library(marginaleffects)
library(lindia)
library(emmeans)
library(gtsummary)
library(mlbench)
library(rms)
data <- read_csv("Data.csv")
```

### 1. Logistic Regression - The Machine Learning Way
 
Here we are going to take exactly the same logistic regression problem we used in the Week 4 data work and do the same analysis using a machine learning way of thinking rather than a statistics/epi way of of thinking. Same exact data, different approach. This is a supervised learning classification example. There are other classifiers and we could use decision tress or random forest for this problem if we wanted. 

#### Variable selection

In general machine learning does really care about which variables go into your model, rather we care about how the model as a whole performs in training versus testing. 

### 2. Research question and data

Our research question is:  

- **Can we develop a model that will predict type 2 diabetes**

We have created a DAG and identified that the following factors are associated with type 2 diabetes:   

- `No varaible in data` = Have prediabetes
- `PM_BMI_SR` = Are overweight
- `SDC_AGE_CALC` = Are 45 years or older
- `No varaible in data` = Have a parent, brother, or sister with type 2 diabetes
- `PA_LEVEL_LONG` = Are physically active less than 3 times a week
- `diabetes == "Gestational"` = Have ever had gestational diabetes (diabetes during pregnancy) or given birth to a baby who weighed over 9 pounds
- `SDC_EB_ABORIGINAL` + `SDC_EB_LATIN` + `SDC_EB_BLACK` = Are an African American, Hispanic or Latino, American Indian, or Alaska Native person
- `DIS_LIVER_FATTY_EVER` = Have non-alcoholic fatty liver disease

Let's simplify the dataset so we are not working with so many variables. 

```{r}
data_working <- select(data, "DIS_DIAB_TYPE", "PM_BMI_SR", "SDC_AGE_CALC", "PA_LEVEL_SHORT", "SDC_EB_ABORIGINAL", "SDC_EB_LATIN", "SDC_EB_BLACK", "DIS_LIVER_FATTY_EVER", "SDC_MARITAL_STATUS", "SDC_EDU_LEVEL", "SDC_INCOME", "HS_GEN_HEALTH", "NUT_VEG_QTY", "NUT_FRUITS_QTY", "ALC_CUR_FREQ", "SDC_BIRTH_COUNTRY", "PA_SIT_AVG_TIME_DAY", "SMK_CIG_STATUS", "SLE_TIME", "DIS_DIAB_FAM_EVER")

rm(data) ### Remove the old data from working memory
```

#### Outcome variable

Let's look at the outcome variable, recode, and drop observations that are not relevant. We know that the GLM function needs a 0/1 variable and we want to recode that way now so we don't need to change it after. We also know we want to keep our gestational diabetes variable because we need it later. 

```{r}
table(data_working$DIS_DIAB_TYPE)

data_working <- data_working %>%
	mutate(diabetes_t2 = case_when(
    DIS_DIAB_TYPE == 2 ~ 1,
    DIS_DIAB_TYPE == -7 ~ 0, 
		TRUE ~ NA_real_
	))

data_working$diabetes_t2 <- as.factor(data_working$diabetes_t2)

table(data_working$diabetes_t2, data_working$DIS_DIAB_TYPE)

data_working <- data_working %>%
	mutate(diabetes_gestat = case_when(
    DIS_DIAB_TYPE == 3 ~ 1,
    DIS_DIAB_TYPE == -7 ~ 0, 
		TRUE ~ NA_real_
	))

data_working$diabetes_gestat <- as.factor(data_working$diabetes_gestat)

data_working <- filter(data_working, diabetes_t2 == 0 | diabetes_t2 == 1 | diabetes_gestat == 1)

table(data_working$diabetes_t2, data_working$DIS_DIAB_TYPE)

data_working <- data_working %>%
	mutate(diabetes = case_when(
    diabetes_t2 == 0 ~ "neg",
    diabetes_t2 == 1 ~ "pos"
	))

table(data_working$diabetes_t2, data_working$diabetes)
```

For logistic regression in the case of a cross-section study we want the outcome to be ~10% of the total sample. Here we have `2160/36807*100 = 5.86%`. 

#### Preparing predictor variables

**BMI overweight**

```{r}
glimpse(data_working$PM_BMI_SR)

summary(data_working$PM_BMI_SR) ### Lots of NAs! 

data_working <- data_working %>%
	mutate(bmi_overweight = case_when(
	  PM_BMI_SR >= 25.00 ~ "Overweight",
		PM_BMI_SR < 25.00 ~ "Not Overweight"
	))

table(data_working$bmi_overweight)
```

**Age**

```{r}
glimpse(data_working$SDC_AGE_CALC)

summary(data_working$SDC_AGE_CALC) ### Lots of NAs! 

data_working <- data_working %>%
	mutate(age_45 = case_when(
	  SDC_AGE_CALC >= 45.00 ~ "Over 45",
		SDC_AGE_CALC < 45.00 ~ "Under 45"
	))

table(data_working$age_45)
```

**Physical Activity**

```{r}
glimpse(data_working$PA_LEVEL_SHORT)

table(data_working$PA_LEVEL_SHORT)

data_working <- data_working %>%
	mutate(pa_cat = case_when(
		PA_LEVEL_SHORT == 1 ~ "1_Low Activity",
		PA_LEVEL_SHORT == 2 ~ "2_Moderate Activity",
		PA_LEVEL_SHORT == 3 ~ "3_High Activity"
	))

table(data_working$pa_cat, data_working$PA_LEVEL_SHORT)
```

**Racialized**

```{r}
table(data_working$SDC_EB_ABORIGINAL)
table(data_working$SDC_EB_LATIN)
table(data_working$SDC_EB_BLACK)

### Latinx

data_working <- data_working %>%
	mutate(latinx = case_when(
		SDC_EB_LATIN == 1 ~ "Yes",
		SDC_EB_LATIN == 0 ~ "No"
	))

table(data_working$SDC_EB_LATIN, data_working$latinx)

### Indigenous

data_working <- data_working %>%
	mutate(indigenous = case_when(
		SDC_EB_ABORIGINAL == 1 ~ "Yes",
		SDC_EB_ABORIGINAL == 0 ~ "No"
	))

table(data_working$SDC_EB_ABORIGINAL, data_working$indigenous)

### Black

data_working <- data_working %>%
	mutate(black = case_when(
		SDC_EB_BLACK == 1 ~ "Yes",
		SDC_EB_BLACK == 0 ~ "No"
	))

table(data_working$SDC_EB_BLACK, data_working$black)
```

**Fatty liver disease**

```{r}
table(data_working$DIS_LIVER_FATTY_EVER)

data_working <- data_working %>%
	mutate(fatty_liver = case_when(
		DIS_LIVER_FATTY_EVER == 1 ~ "Yes",
		DIS_LIVER_FATTY_EVER == 2 ~ "Yes"
	))

data_working <- data_working %>%
	mutate(fatty_liver = case_when(
		DIS_LIVER_FATTY_EVER == 1 ~ "Yes",
		DIS_LIVER_FATTY_EVER == 2 ~ "Yes"
	))

data_working <- data_working %>% 
                  mutate(fatty_liver = replace_na(fatty_liver, "No"))

table(data_working$fatty_liver)
```

#### 3. Preliminary analysis and setup

We are going to use the [Tidymodels](https://www.tidymodels.org/) framework for this analysis since we are in the Tidyverse world for this course. On our previous examples for logistic and linear regression we did not use the tidyverse framework. 

Removing missing data. Normally you would do this variable by variable and be very clear on what and why you are removing data. We are just going to simplify that here. 

```{r}
cols <- c("pa_cat", "latinx", "indigenous", "black", "fatty_liver", "SDC_MARITAL_STATUS", "SDC_EDU_LEVEL", "SDC_INCOME", "HS_GEN_HEALTH", "diabetes", "SDC_BIRTH_COUNTRY", "SMK_CIG_STATUS", "DIS_DIAB_FAM_EVER")
data_working %<>% mutate_at(cols, factor)

data_working <- select(data_working, diabetes_t2, diabetes, PM_BMI_SR, SDC_AGE_CALC, pa_cat, latinx, indigenous, black, fatty_liver, SDC_MARITAL_STATUS, SDC_EDU_LEVEL, SDC_INCOME, HS_GEN_HEALTH, NUT_VEG_QTY, NUT_FRUITS_QTY, ALC_CUR_FREQ, SDC_BIRTH_COUNTRY, PA_SIT_AVG_TIME_DAY, SMK_CIG_STATUS, SLE_TIME, DIS_DIAB_FAM_EVER)

data_working <- data_working %>% drop_na()
```

#### 3.1 Data Split

More machine learning we need a way to split the data into a training set and a test set. There are a few different approaches too this. Here we are going to use an 70/30 split with 70% of the data going to training and 30 going to testing. This is sort of an older way to split data and I would say that a k-fold cross validation is probably more in line with modern practice... but we are here for the learning. 

```{r}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible when random numbers are used 
set.seed(1)

data_split <- initial_split(data_working, prop = 0.70, strata = diabetes)

# Create data frames for the two sets:
train_data <- training(data_split)

table(train_data$diabetes_t2)

test_data  <- testing(data_split)

table(test_data$diabetes_t2)
```

Now we have split the data, we want to create the model for the training data and save it so it can be applied to the testing set. This is basically exactly what we did before. __Note that we only run the model on the training data__ Not all of the data like would in a traditional logistic regression. Here we won't get the exact same result as our original logistic regression because we don't have the same data. We expect there will be some variation but that the results should relatively similar. 

**Another note. I've added variables to this model compared to our previous model. The previous model did a very poor job of predicting diabetes overall. In fact, it had a sensitivity of ZERO! Meaning it did not predict a single case of diabetes in the test set. That's bad so I've added variables to try and increase our prediction ability. 

#### 3.2 Running the regression

```{r}
logistic_model<- logistic_reg() %>%
        set_engine("glm") %>%
        set_mode("classification") %>%
        fit(diabetes ~ PM_BMI_SR + SDC_AGE_CALC + pa_cat + latinx + indigenous + black + fatty_liver + SDC_MARITAL_STATUS + SDC_EDU_LEVEL + SDC_INCOME + HS_GEN_HEALTH + NUT_VEG_QTY + NUT_FRUITS_QTY + ALC_CUR_FREQ + SDC_BIRTH_COUNTRY + PA_SIT_AVG_TIME_DAY + SMK_CIG_STATUS + SLE_TIME + DIS_DIAB_FAM_EVER, data = train_data)

tidy(logistic_model, exponentiate = TRUE)
```

#### 3.3 Test the trained model

Once we `train the model` we want to understand how well our trained model works on new data the model has not seen. This is where the testing data comes in. We can use the `predict` feature for this. What we are doing here is predicting if someone has diabetes (yes/no) from the model we trained using the training data, on the testing data. We had 4293 observations in the training with 4077 people with on diabetes and 216 people with diabetes. Much of this example comes from [https://medium.com/the-researchers-guide/modelling-binary-logistic-regression-using-tidymodels-library-in-r-part-1-c1bdce0ac055](https://medium.com/the-researchers-guide/modelling-binary-logistic-regression-using-tidymodels-library-in-r-part-1-c1bdce0ac055)

The code below outputs the predict class `diabetes (yes/no)` for the test data. 

```{r}
pred_class <- predict(logistic_model,
                      new_data = test_data,
                      type = "class")
table(pred_class$.pred_class)
```

Our model predicts that we have 4287 people with diabetes and 6 people with diabetes. Not looking good for our model! 

Now we want to generated the predicted probabilities for the model. That is, how well does our model think it does for each person. 

```{r}
pred_prob <- predict(logistic_model,
                      new_data = test_data,
                      type = "prob")
head(pred_prob)
```

This is not very informative in terms of results but we will discuss this more later. 

Now we want to combine all of our results into one dataframe and just do a quick check. 

```{r}
diabetes_results <- test_data %>%
  select(diabetes_t2, diabetes) %>%
  bind_cols(pred_class, pred_prob)

head(diabetes_results)
```

Here we can see the first 6 rows of data data all negative for diabetes and are predicted as negative. The model is very confident in these predictions, with over 90% negative prediction in all six observations. 

#### 3.3 Model evaluation

There are a number of different methods we must use to evaluate machine learning models. We will walk through those. 

#### Calibration Slope

The calibration slope is the predicted values from the model compared to the actual values [https://doi.org/10.1016/j.jclinepi.2019.09.016](https://doi.org/10.1016/j.jclinepi.2019.09.016) 

```{}


p <- diabetes_results$.pred_neg
y <- diabetes_results$diabetes_t2


val.prob(p, y)



set.seed(1)
n <- 200
x1 <- runif(n)
x2 <- runif(n)
x3 <- runif(n)
logit <- 2*(x1-.5)
P <- 1/(1+exp(-logit))
y <- ifelse(runif(n)<=P, 1, 0)
d <- data.frame(x1,x2,x3,y)
f <- lrm(y ~ x1 + x2 + x3, subset=1:100)
pred.logit <- predict(f, d[101:200,])
phat <- 1/(1+exp(-pred.logit))
val.prob(phat, y[101:200], m=20, cex=.5)  # subgroups of 20 obs.


# Validate predictions more stringently by stratifying on whether
# x1 is above or below the median


v <- val.prob(phat, y[101:200], group=x1[101:200], g.group=2)
v
plot(v)

```



#### Confusion Matrix

We can generate a confusion matrix by using the `conf_mat()` function by supplying the final data frame (`diabetes_results`), the truth column `diabetes` and predicted class `.pred_class` in the estimate attribute.

A confusion matrix is sort of a 2x2 table with the true values on one side and predicted values in another column. If we look on the diagonal we see when the model correctly predicts the values `yes/no` and off diagonal is when the model does not predict the correct value. So this model correctly predicts that 4075 cases of diabetes and incorrectly predicts that 212 people do not have diabetes when they do have it. The model correctly predicts 4 cases of diabetes. It also incorrectly predicts that two people who do not have diabetes do have diabetes. 

```{r}
conf_mat(diabetes_results, truth = diabetes,
         estimate = .pred_class)
```

#### Accuracy

We can calculate the classification accuracy by using the `accuracy()` function by supplying the final data frame `diabetes_results`, the truth column `diabetes` and predicted class `.pred_class` in the estimate attribute. The model classification accuracy on test dataset is about 95.0%. This looks good but it's a bit of fake result as we will see later. 

```{r}
accuracy(diabetes_results, truth = diabetes,
         estimate = .pred_class)
```

#### Sensitivity

The sensitivity (also known as __Recall__) of a classifier is the ratio between what was correctly identified as positive (True Positives) and all positives (False Negative + True Positive).

__Sensitivity = TP / FN + TP__

The sensitivity value is 1.0 indicating that we are able to correctly detect 100% of the positive values.  

```{r}
sens(diabetes_results, truth = diabetes,
    estimate = .pred_class)
```

#### Specificity

Specificity of a classifier is the ratio between what was classified as negative (True Negatives) and all negative values (False Positive + True Native)

__Specificity = TN / FP + TN__

The specificity value is 0.0185. Meaning that we correctly classify 1.85% of the negative values, which is pretty terrible. 

```{r}
spec(diabetes_results, truth = diabetes,
    estimate = .pred_class)
```

#### Precision

What percent of values are correctly classified as positive (True Positives) out of all positives (True Positive + False Positive)?

__Precision = TP / TP + FP__

The precision is 0.818, meaning we identify 81.8% of true positives compared to all positives. 

```{r}
precision(diabetes_results, truth = diabetes,
    estimate = .pred_class)
```

#### F-Score

F-score is the mean of precision and sensitivity. The value ranges from 1 (the best score) and 0 (the worst score). F-score gives us the balance between precision and sensitivity. The F1 score is about 0.974, which indicates that the trained model has a classification strength of 07.4%.

```{r}
f_meas(diabetes_results, truth = diabetes,
       estimate = .pred_class)
```

#### ROC Curve

The ROC curve is plotted with `sensitivity` against `1 - Specificity`, where `sensitivity` is on the y-axis and `1 - Specificity` is on the x-axis. A line is drawn diagonally to denote 50–50 partitioning of the graph. If the curve is more close to the line, lower the performance of the classifier, which is no better than a mere random guess.

You can generate a ROC Curve using the `roc_curve()` function where you need to supply the truth column `diabetes` and predicted probabilities for the positive class `.pred_pos`.

Our model has got a ROC-AUC score of 0.243 indicating a good model that cannot distinguish between patients with diabetes and no diabetes.

```{r}
roc_auc(diabetes_results,
        truth = diabetes,
        .pred_pos)

roc_curve <- diabetes_results %>%
  roc_curve(truth = diabetes, .pred_pos) %>%
  autoplot()

plot(roc_curve)
```





#### All the metrics 

We can produce all of the metrics using the `metric_set` function. 

```{r}
metrics <- metric_set(accuracy, sens, spec, precision, recall, f_meas)

all_metrics <- metrics(diabetes_results,
               truth = diabetes,
               estimate = .pred_class)
               
kable(all_metrics)
```




#### 3.4 Model interpretation


